---
layout: inner
position: left
title: 'RL applied to Self-Driving Race Cars'
date: 2018-09-01 15:56:00
categories: RL TRPO
type: related
tags: RL TRPO ROS
featured_image: 'img/posts/EUFS-RL/eufs-ddpg.png'
project_link: 'https://gitlab.com/eufs/ddpg_racecar'
button_icon: 'github'
button_text: 'Visit Project'
lead_text: 'Using TRPO in order to solve EUFS problem (Driving around a track)'
author: Mike
---

# Using TRPO to solve Driving Task around a Track

First thing to mention is the other authors of this project, Ignat Georgiev and Johny De Bod. **You can see the final report [here](https://drive.google.com/file/d/18o5bm2fVjuamXqJIQX0y_xsCaCi3Cik1/view?usp=sharing){:target="_blank"}**

The Formula Student Challenge ([link](https://www.imeche.org/events/formula-student)) is a competition where the idea is to build from zero a F1 race car. One of its branches is autonomous driving, where the idea is to make the car drive itself around a track made of traffic cones. The usual solutions come form a clasical approach made by Perception, Mapping and Planning subsystems. But we decided to solve the problem using End-to-End Reinforcement Learning as a course project.

## Environment

The environment is made using ROS and Gazebo simulation, where we have the model of the team's car. The reason behind it is that we are interesting in mimiquing the same architecture used for our F1 car, therefore we have the same components in theory as the car in real life, in order to apply the code easier to the car. 

![img](/site/img/posts/EUFS-RL/eufs-ddpg.png)

The agents is equiped with different sensors, mainly one Lidar and one stereo camera, but for this task we decided to use only one monucular camera follwing the architecture of this paper ([link](https://arxiv.org/abs/1807.00412){:target="_blank"}). The tracks are made of yellow and blue traffic cones for each side of the track.

## Summary

After getting Gazebo ready, building a gym API on top of it, exploring DDPG, PPO and TRPO,  and several days of training, we manage to solve partially the problem. Given the time constrains we had, we decided to leave one action out of the equation, we removed one degree of freedom, we made acceleration constant and focused on learning a controller for steering. We finally, after recollecting around 130GB of logs just on experiments, managed to get a RL model which learning a very sub-optimal policy, but nontheless a policy capable of doing several laps around the track.

## Conclusion

Although we didn't get the results we were expenting, we had to explore and learn so many things to get there, that we all were happy with the result. From there we have learnt so many things and I am sure now, if we had to repeat this project, we will tweak it to get right key aspects that we missed the first time.

## Video 

In the following video you can see the environment from a top-view. To the left the cost map function can be seen, and to the right the realtime observations of the agent as it advances on the track. The policy is not very robust 

<iframe width="560" height="315" src="https://www.youtube.com/embed/j61t9nLWuUQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen">  </iframe>