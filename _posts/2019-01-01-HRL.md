---
layout: inner
position: left
title: 'Hierarchical XAI systems for RL in Autonomous Vehicles'
date: 2019-01-01 14:15:00
categories: development
type: related
tags: Hierarchical-RL Python DQN
featured_image: 'img/posts/HRL/hrl-c.png'
project_link: 'https://github.com/notanymike/HRL'
button_icon: 'github'
button_text: 'Visit Project'
lead_text: 'Using Hierarchical Policies in order to improve the general performance of RL models'


---

# Hierarchical XAI systems for RL in Autonomous Vehicles

MSc Thesis / University of Edinburgh[^1][^2]

In general, a Hierarchical Reinforcement Learning is a group of RL models organised in a hierarchy. Every model is specialised in certain actions, and there is one model which takes care of solving the main task, usually called the “controller”. Given the hierarchy organisation, every model in a higher level uses some of the policies (or models) in lower levels to accomplish its specialised task. More specifically there are many different types of HRL models, some specialised in learning the policies, some others in learning the hierarchy, etc (to see a more in-depth description of HRL see this interesting article from The Gradient, [link](https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning){:target="_blank"}). Each one uses different concepts to solve the problem. In this case, I am focusing on pre-trained pre-specified hierarchy to test different characteristics of such models. 

Take for example the hierarchy below (see figure 1) to drive a car in a simplified environment, here the policy “Avoid Obstacle” can make use of the sub-policies “Change Lane” or “Stay in Lane” to accomplish its goal. In the end, every policy uses a sub-policy or directly the raw actions of the agent, in this case, accelerate, break, turn left, right or do nothing.

![Hierarchy](/site/img/posts/HRL/Hierarchy.png)

There are several benefits to use HRL. We focus on the gains in convergence, that is how good the policies are compared to an End-to-End model, and also the gains in modularity coming from using a Hierarchy of Policies, for example, if the environment changes, the whole hierarchy of policies do not have to change as well, just the affected part, which can mean significant improvements in training times depending on the complexity. Another example is to improve its performance, suppose we found a policy which is not behaving as expected in a specific case, the solution is to modify somehow (e.g. more training) that policy only, and all the other policies stay the same in the hierarchy.

------

[^1]: Currently working on this project, I will keep updating this post based on the progress of the thesis.
[^2]: The cover picture is taken from [here](<https://arxiv.org/pdf/1710.09767.pdf>){:target="_blank"}